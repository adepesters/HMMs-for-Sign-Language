{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recognizing Sign Language Using HMMs\n",
    "- [Introduction](#intro)\n",
    "- [Part 1: Feature Selection](#part1)\n",
    "- [Part 2: HMMs Training](#part2)\n",
    "- [Part 3: HMMs Testing](#part3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "In this project, I build a word recognizer for American Sign Language (ASL) video sequences using Hidden Markov Models.  I use the [RWTH-BOSTON-104 Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database-rwth-boston-104.php), which consists of 201 videos of ASL sentences annotated with the x-y positions of the speaker's nose tip, left and right hands. \n",
    "\n",
    "The HMMs are trained using the [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/) library. \n",
    "\n",
    "This notebook is composed of three parts. In Part 1, I derive a variety of feature sets. In Part 2, I implement three different model selection criteria to determine the optimal number of hidden states for each word model. Finally, in Part 3, I train, test and compare the different models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part1'></a>\n",
    "## PART 1: Feature Selection\n",
    "\n",
    "Each data frame has been annotated with the speaker's nose, left and right hand position. As features, we will try a combination of cartesian and polar coordinates as well as the changes of hand position between time frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Loading the database and looking at one particular example of data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left-x         155\n",
       "left-y         188\n",
       "right-x        171\n",
       "right-y        164\n",
       "nose-x         169\n",
       "nose-y          56\n",
       "speaker    woman-2\n",
       "Name: (37, 98), dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.model_selection import KFold\n",
    "from asl_utils import combine_sequences\n",
    "from asl_data import AslDb\n",
    "from asl_data import SinglesData\n",
    "from asl_utils import show_errors\n",
    "\n",
    "asl = AslDb() # initializes the database using the AslDb class in the asl_data module\n",
    "asl.df.ix[37,98]  # look at the data available for an individual frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This corresponds to the following frame: ![Video 98](http://www-i6.informatik.rwth-aachen.de/~dreuw/database/rwth-boston-104/overview/images/orig/037-end.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "I first compute the scaled cartesian coordinates of the left and right hands as first set of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaled_cartesian = ['cartesian-rx', 'cartesian-ry', 'cartesian-lx', 'cartesian-ly']\n",
    "\n",
    "asl.df['cartesian-rx'] = (asl.df['right-x'] - asl.df['right-x'].min()) / (asl.df['right-x'].max() - asl.df['right-x'].min())\n",
    "asl.df['cartesian-lx'] = (asl.df['left-x'] - asl.df['left-x'].min()) / (asl.df['left-x'].max() - asl.df['left-x'].min())\n",
    "asl.df['cartesian-ry'] = (asl.df['right-y'] - asl.df['right-y'].min()) / (asl.df['right-y'].max() - asl.df['right-y'].min())\n",
    "asl.df['cartesian-ly'] = (asl.df['left-y'] - asl.df['left-y'].min()) / (asl.df['left-y'].max() - asl.df['left-y'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As second set of features, I chose the normalized polar coordinates of the hands referenced to the nose: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we first compute the hands coordinates referenced to the nose\n",
    "\n",
    "asl.df['grnd-ry'] = asl.df['right-y'] - asl.df['nose-y']\n",
    "asl.df['grnd-rx'] = asl.df['right-x'] - asl.df['nose-x']\n",
    "asl.df['grnd-ly'] = asl.df['left-y'] - asl.df['nose-y']\n",
    "asl.df['grnd-lx'] = asl.df['left-x'] - asl.df['nose-x']\n",
    "\n",
    "df_means = asl.df.groupby('speaker').mean()\n",
    "df_std = asl.df.groupby('speaker').std()\n",
    "\n",
    "norm_polar = ['polar-rr', 'polar-rtheta', 'polar-lr', 'polar-ltheta']\n",
    "\n",
    "asl.df['polar-rr'] = np.sqrt(np.square(asl.df['grnd-rx']) + np.square(asl.df['grnd-ry']))\n",
    "asl.df['polar-rtheta'] = np.arctan2(asl.df['grnd-rx'], asl.df['grnd-ry'])\n",
    "\n",
    "asl.df['polar-lr'] = np.sqrt(np.square(asl.df['grnd-lx']) + np.square(asl.df['grnd-ly']))\n",
    "asl.df['polar-ltheta'] = np.arctan2(asl.df['grnd-lx'], asl.df['grnd-ly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As third set of features, I compute the difference of hand position between every 2 frames: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "delta_2 = ['delta_2-rx', 'delta_2-ry', 'delta_2-lx', 'delta_2-ly']\n",
    "\n",
    "asl.df['delta_2-rx'] = asl.df['right-x'].diff(2)\n",
    "asl.df['delta_2-ry'] = asl.df['right-y'].diff(2)\n",
    "asl.df['delta_2-lx'] = asl.df['left-x'].diff(2)\n",
    "asl.df['delta_2-ly'] = asl.df['left-y'].diff(2)\n",
    "\n",
    "asl.df['delta_2-rx'].fillna(0, inplace = True)\n",
    "asl.df['delta_2-ry'].fillna(0, inplace = True)\n",
    "asl.df['delta_2-lx'].fillna(0, inplace = True)\n",
    "asl.df['delta_2-ly'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, as fourth set of features I compute the difference of hand position between every 3 frames: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "delta_3 = ['delta_3-rx', 'delta_3-ry', 'delta_3-lx', 'delta_3-ly']\n",
    "\n",
    "asl.df['delta_3-rx'] = asl.df['right-x'].diff(3)\n",
    "asl.df['delta_3-ry'] = asl.df['right-y'].diff(3)\n",
    "asl.df['delta_3-lx'] = asl.df['left-x'].diff(3)\n",
    "asl.df['delta_3-ly'] = asl.df['left-y'].diff(3)\n",
    "\n",
    "asl.df['delta_3-rx'].fillna(0, inplace = True)\n",
    "asl.df['delta_3-ry'].fillna(0, inplace = True)\n",
    "asl.df['delta_3-lx'].fillna(0, inplace = True)\n",
    "asl.df['delta_3-ly'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And combining all features.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = scaled_cartesian + norm_polar + delta_2 + delta_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part2'></a>\n",
    "## PART 2: HMMs Training\n",
    "\n",
    "I determine the optimal number of states for each word model using three different selection criteria: \n",
    "- Log likelihood using cross-validation folds (CV)\n",
    "- Bayesian Information Criterion (BIC)\n",
    "- Discriminative Information Criterion (DIC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Implementing the base class for model selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ModelSelector(object):\n",
    "    '''\n",
    "    base class for model selection\n",
    "    '''\n",
    "\n",
    "    def __init__(self, all_word_sequences: dict, all_word_Xlengths: dict, this_word: str,\n",
    "                 n_constant=3,\n",
    "                 min_n_components=2, max_n_components=10,\n",
    "                 random_state=14, verbose=False):\n",
    "        self.words = all_word_sequences\n",
    "        self.hwords = all_word_Xlengths\n",
    "        self.sequences = all_word_sequences[this_word]\n",
    "        self.X, self.lengths = all_word_Xlengths[this_word]\n",
    "        self.this_word = this_word\n",
    "        self.n_constant = n_constant\n",
    "        self.min_n_components = min_n_components\n",
    "        self.max_n_components = max_n_components\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Implementing selector according to the best averaged log-likelihood over k-fold cross-validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SelectorCV(ModelSelector):\n",
    "    ''' select best model based on average log Likelihood of cross-validation folds\n",
    "    '''\n",
    "\n",
    "    def select(self):\n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "        number_splits= min(len(self.sequences), 3)\n",
    "        if number_splits > 1:\n",
    "            split_method = KFold(n_splits=number_splits)\n",
    "            max_logL = float('-Inf')\n",
    "\n",
    "            for num_states in range(self.min_n_components, self.max_n_components+1):\n",
    "                \n",
    "                current_logL = 0\n",
    "\n",
    "                try:\n",
    "                    for cv_train_idx, cv_test_idx in split_method.split(self.sequences):\n",
    "\n",
    "                        X_train, lengths_train = combine_sequences(cv_train_idx, self.sequences)\n",
    "                        X_test, lengths_test = combine_sequences(cv_test_idx, self.sequences)\n",
    "\n",
    "                        model = GaussianHMM(n_components=num_states, covariance_type=\"diag\", n_iter=1000,\n",
    "                                            random_state=self.random_state, verbose=False).fit(X_train, lengths_train)\n",
    "\n",
    "                        logL = model.score(X_test, lengths_test)\n",
    "\n",
    "                        current_logL += logL\n",
    "\n",
    "                    current_logL = current_logL/number_splits\n",
    "                    \n",
    "                    if current_logL > max_logL:\n",
    "                        \n",
    "                        max_logL = current_logL\n",
    "                        best_num_components = num_states\n",
    "                        best_model = model\n",
    "                        \n",
    "                except:  \n",
    "                    continue\n",
    "            \n",
    "        if 'best_model' not in locals():    \n",
    "            best_model = None\n",
    "\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Implementing the BIC model selector, i.e., selects the model with lowest BIC value, where BIC= -2 * logL + p * logN, with logL = log-likelihood of the model, p = the number of parameters of the model, and N = the number of observations. BIC penalizes models with a high number of states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SelectorBIC(ModelSelector):\n",
    "    \"\"\" select the model with the lowest Bayesian Information Criterion(BIC) score\n",
    "    \"\"\"\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\" select the best model for self.this_word based on\n",
    "        BIC score for n between self.min_n_components and self.max_n_components\n",
    "\n",
    "        :return: GaussianHMM object\n",
    "        \"\"\"\n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "        \n",
    "        lowest_BIC_score = float('Inf')\n",
    "        \n",
    "        for num_states in range(self.min_n_components, self.max_n_components+1):\n",
    "            \n",
    "            try:\n",
    "           \n",
    "                model = GaussianHMM(n_components=num_states, covariance_type=\"diag\", n_iter=1000,\n",
    "                                    random_state=self.random_state, verbose=False).fit(self.X, self.lengths)\n",
    "                    \n",
    "                logL = model.score(self.X, self.lengths)\n",
    "                    \n",
    "                    # nb_params = nb of states * (number of states -1) (i.e., transition probabilities from state to state) \n",
    "                    # + number of states - 1 (i.e., initial probabilities of being in a particular state)\n",
    "                    # + number of states * number of parameters to model each feature, (i.e., emission probabilities or probability to observe a feature given a state), i.e. number of states * number of features * 2 (i.e. 2 counting for mean and standard deviation of each gaussian for each feature)\n",
    "                    # --> finally, nb_params = nb_states * (nb_states -1) + nb_states -1 + nb_states * nb_features * 2 = nb_states^2 - 1 + nb_states*nb_features*2 \n",
    "                    \n",
    "                num_params = num_states * num_states - 1 + 2 * num_states * len(self.X[0])\n",
    "                num_frames = len(self.X)\n",
    "                    \n",
    "                current_BIC_score = num_params * math.log(num_frames) - 2 * logL\n",
    "              \n",
    "                if current_BIC_score < lowest_BIC_score:\n",
    "                  \n",
    "                    lowest_BIC_score = current_BIC_score\n",
    "                  \n",
    "                    best_model = model\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if 'best_model' not in locals():    \n",
    "            best_model = None\n",
    "              \n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Implementing the DIC model selector, i.e., selects the model with highest DIC value. DIC maximizes the discrimination between classes by computing the difference between the logL of the model on the trained word and the averaged logL of the model on all other words. It is computed according to Biem, Alain. \"A model selection criterion for classification: Application to hmm topology optimization.\" Document Analysis and Recognition, 2003. Proceedings. Seventh International Conference on. IEEE, 2003:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SelectorDIC(ModelSelector):\n",
    "    ''' select best model based on Discriminative Information Criterion    \n",
    "    '''\n",
    "\n",
    "    def select(self):\n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "        \n",
    "        highest_DIC_score = float('-Inf')\n",
    "        \n",
    "        for num_states in range(self.min_n_components, self.max_n_components+1):\n",
    "            \n",
    "            logL_antievidence = 0\n",
    "            \n",
    "            try:\n",
    "                           \n",
    "                model = GaussianHMM(n_components=num_states, covariance_type=\"diag\", n_iter=1000,\n",
    "                                    random_state=self.random_state, verbose=False).fit(self.X, self.lengths)\n",
    "                    \n",
    "                logL_evidence = model.score(self.X, self.lengths)\n",
    "                \n",
    "                for key in self.words.keys():\n",
    "                    if key != self.this_word:\n",
    "                        X_test, lengths_test = self.hwords[key]\n",
    "                        logL_antievidence += model.score(X_test, lengths_test)\n",
    "                    \n",
    "                current_DIC_score = logL_evidence - (logL_antievidence / (len(self.words) - 1))\n",
    "                \n",
    "                if current_DIC_score > highest_DIC_score:\n",
    "                  \n",
    "                    highest_DIC_score = current_DIC_score\n",
    "                    \n",
    "                    best_model = model\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if 'best_model' not in locals():    \n",
    "            best_model = None\n",
    "              \n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part3'></a>\n",
    "## PART 3: HMMs Testing\n",
    "\n",
    "Here I test the HMMs on a testing set composed of ~40 sentences. Error rate on the vocabulary of 112 trained words is of 50% for the cross-validation-selected models, 45% for BIC, and 48% for DIC.\n",
    "\n",
    "I first implement the train_all_words function that builds the training set and trains and selects an HMM for each word of the training set according to the selected selection criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_all_words(features, model_selector):\n",
    "    training = asl.build_training(features)\n",
    "    sequences = training.get_all_sequences()\n",
    "    Xlengths = training.get_all_Xlengths()\n",
    "    model_dict = {}\n",
    "    for word in training.words:\n",
    "        model = model_selector(sequences, Xlengths, word, \n",
    "                        n_constant=3).select()\n",
    "        model_dict[word]=model\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Building test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = asl.build_test(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here I implement a function that outputs the log-likelihood of each word for each test sequence, along with the word with the highest log-likelihood value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def recognize(models: dict, test_set: SinglesData):\n",
    "    \"\"\" Recognize test word sequences from word models set\n",
    "\n",
    "   :param models: dict of trained models\n",
    "       {'SOMEWORD': GaussianHMM model object, 'SOMEOTHERWORD': GaussianHMM model object, ...}\n",
    "   :param test_set: SinglesData object\n",
    "   :return: (list, list)  as probabilities, guesses\n",
    "       both lists are ordered by the test set word_id\n",
    "       probabilities is a list of dictionaries where each key a word and value is Log Likelihood\n",
    "           [{SOMEWORD': LogLvalue, 'SOMEOTHERWORD' LogLvalue, ... },\n",
    "            {SOMEWORD': LogLvalue, 'SOMEOTHERWORD' LogLvalue, ... },\n",
    "            ]\n",
    "       guesses is a list of the best guess words ordered by the test set word_id\n",
    "           ['WORDGUESS0', 'WORDGUESS1', 'WORDGUESS2',...]\n",
    "   \"\"\"\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "       \n",
    "    probabilities = []\n",
    "    guesses = []\n",
    "    for word_id in range(test_set.num_items):\n",
    "        X_test, lengths_test = test_set.get_item_Xlengths(word_id)\n",
    "        logL = {}\n",
    "        for key in models.keys():\n",
    "            try:\n",
    "                logL[key] = models[key].score(X_test, lengths_test)\n",
    "            except:\n",
    "                continue\n",
    "        probabilities.append(logL)\n",
    "        best_guess = max(logL, key=logL.get)\n",
    "        guesses.append(best_guess)\n",
    "\n",
    "    return probabilities, guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, I test the models. \n",
    "\n",
    "First, testing the model that uses cross-validation to select the optimal number of states. Results are displayed as follows: ground truth for each sentence is on the right column, and recognized words on the left. Errors are indicated with an asterisk (WER: word error rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** WER = 0.5\n",
      "Total correct: 89 out of 178\n",
      "Video  Recognized                                                    Correct\n",
      "=====================================================================================================\n",
      "    2: *POSS *NEW *ARRIVE                                            JOHN WRITE HOMEWORK\n",
      "    7: JOHN *HAVE GO *HAVE                                           JOHN CAN GO CAN\n",
      "   12: JOHN CAN *SOMETHING-ONE CAN                                   JOHN CAN GO CAN\n",
      "   21: JOHN *JOHN WONT *JOHN *CAR *CAR *FUTURE *FUTURE               JOHN FISH WONT EAT BUT CAN EAT CHICKEN\n",
      "   25: JOHN *IX IX IX IX                                             JOHN LIKE IX IX IX\n",
      "   28: *MARY *IX IX IX IX                                            JOHN LIKE IX IX IX\n",
      "   30: *IX LIKE IX IX IX                                             JOHN LIKE IX IX IX\n",
      "   36: MARY *MARY *SOMETHING-ONE IX *IX *MARY                        MARY VEGETABLE KNOW IX LIKE CORN1\n",
      "   40: JOHN IX *CORN MARY *MARY                                      JOHN IX THINK MARY LOVE\n",
      "   43: JOHN *POSS BUY HOUSE                                          JOHN MUST BUY HOUSE\n",
      "   50: *MARY *POSS BUY CAR *MARY                                     FUTURE JOHN BUY CAR SHOULD\n",
      "   54: JOHN *JOHN *IX BUY HOUSE                                      JOHN SHOULD NOT BUY HOUSE\n",
      "   57: JOHN *MARY VISIT *IX                                          JOHN DECIDE VISIT MARY\n",
      "   67: JOHN FUTURE *FUTURE BUY HOUSE                                 JOHN FUTURE NOT BUY HOUSE\n",
      "   71: JOHN *JOHN *BLAME MARY                                        JOHN WILL VISIT MARY\n",
      "   74: JOHN *MARY *IX *IX                                            JOHN NOT VISIT MARY\n",
      "   77: *JOHN BLAME MARY                                              ANN BLAME MARY\n",
      "   84: *JOHN *NEW SOMETHING-ONE BOOK                                 IX-1P FIND SOMETHING-ONE BOOK\n",
      "   89: *SOMETHING-ONE IX *IX *IX IX NEW COAT                         JOHN IX GIVE MAN IX NEW COAT\n",
      "   90: *MARY *SOMETHING-ONE *SOMETHING-ONE SOMETHING-ONE WOMAN BOOK  JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   92: JOHN *IX IX *IX *IX BOOK                                      JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "  100: POSS NEW CAR BREAK-DOWN                                       POSS NEW CAR BREAK-DOWN\n",
      "  105: *MARY *MARY                                                   JOHN LEG\n",
      "  107: JOHN *JOHN *HAVE HAVE *MARY                                   JOHN POSS FRIEND HAVE CANDY\n",
      "  108: *IX *LOVE                                                     WOMAN ARRIVE\n",
      "  113: *JOHN CAR *MARY *MARY *BUY1                                   IX CAR BLUE SUE BUY\n",
      "  119: *MARY *BUY1 *GO *HAVE *HAVE                                   SUE BUY IX CAR BLUE\n",
      "  122: JOHN *GIVE1 BOOK                                              JOHN READ BOOK\n",
      "  139: JOHN *BUY1 WHAT *SOMETHING-ONE BOOK                           JOHN BUY WHAT YESTERDAY BOOK\n",
      "  142: JOHN BUY YESTERDAY WHAT BOOK                                  JOHN BUY YESTERDAY WHAT BOOK\n",
      "  158: LOVE JOHN *MARY                                               LOVE JOHN WHO\n",
      "  167: JOHN *JOHN *IX LOVE *VISIT                                    JOHN IX SAY LOVE MARY\n",
      "  171: *MARY *JOHN BLAME                                             JOHN MARY BLAME\n",
      "  174: *HAVE *ARRIVE GIVE1 *MARY *HAVE                               PEOPLE GROUP GIVE1 JANA TOY\n",
      "  181: *SUE *BOX                                                     JOHN ARRIVE\n",
      "  184: *SOMETHING-ONE BOY *GIVE1 TEACHER APPLE                       ALL BOY GIVE TEACHER APPLE\n",
      "  189: JOHN *JOHN *MARY BOX                                          JOHN GIVE GIRL BOX\n",
      "  193: JOHN *JOHN *MARY BOX                                          JOHN GIVE GIRL BOX\n",
      "  199: *JOHN CHOCOLATE WHO                                           LIKE CHOCOLATE WHO\n",
      "  201: JOHN *GIVE *IX *IX BUY HOUSE                                  JOHN TELL MARY IX-1P BUY HOUSE\n"
     ]
    }
   ],
   "source": [
    "model_selector = SelectorCV\n",
    "\n",
    "models = train_all_words(features, model_selector)\n",
    "test_set = asl.build_test(features)\n",
    "probabilities, guesses = recognize(models, test_set)\n",
    "show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Testing the models selected using BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** WER = 0.449438202247191\n",
      "Total correct: 98 out of 178\n",
      "Video  Recognized                                                    Correct\n",
      "=====================================================================================================\n",
      "    2: JOHN WRITE *ARRIVE                                            JOHN WRITE HOMEWORK\n",
      "    7: JOHN CAN GO *ARRIVE                                           JOHN CAN GO CAN\n",
      "   12: JOHN CAN *GO1 CAN                                             JOHN CAN GO CAN\n",
      "   21: JOHN *ARRIVE WONT *JOHN *STUDENT *GIVE1 *FUTURE *MARY         JOHN FISH WONT EAT BUT CAN EAT CHICKEN\n",
      "   25: JOHN LIKE *MARY IX *MARY                                      JOHN LIKE IX IX IX\n",
      "   28: JOHN LIKE IX *JOHN IX                                         JOHN LIKE IX IX IX\n",
      "   30: JOHN LIKE *MARY IX IX                                         JOHN LIKE IX IX IX\n",
      "   36: MARY *JOHN *IX IX *MARY *MARY                                 MARY VEGETABLE KNOW IX LIKE CORN1\n",
      "   40: JOHN IX *CORN MARY *MARY                                      JOHN IX THINK MARY LOVE\n",
      "   43: JOHN *SHOULD BUY HOUSE                                        JOHN MUST BUY HOUSE\n",
      "   50: *JOHN *POSS BUY CAR SHOULD                                    FUTURE JOHN BUY CAR SHOULD\n",
      "   54: JOHN *FUTURE *FUTURE BUY HOUSE                                JOHN SHOULD NOT BUY HOUSE\n",
      "   57: JOHN *JOHN *GIVE *IX                                          JOHN DECIDE VISIT MARY\n",
      "   67: JOHN FUTURE *WHO BUY HOUSE                                    JOHN FUTURE NOT BUY HOUSE\n",
      "   71: JOHN *FUTURE *GO MARY                                         JOHN WILL VISIT MARY\n",
      "   74: *IX *MARY *MARY MARY                                          JOHN NOT VISIT MARY\n",
      "   77: *JOHN BLAME *LOVE                                             ANN BLAME MARY\n",
      "   84: *JOHN *JOHN *GIVE1 BOOK                                       IX-1P FIND SOMETHING-ONE BOOK\n",
      "   89: JOHN *JOHN *IX *WOMAN IX NEW COAT                             JOHN IX GIVE MAN IX NEW COAT\n",
      "   90: JOHN *IX IX *IX WOMAN BOOK                                    JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   92: JOHN *IX IX *WOMAN WOMAN BOOK                                 JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "  100: POSS NEW CAR BREAK-DOWN                                       POSS NEW CAR BREAK-DOWN\n",
      "  105: JOHN *MARY                                                    JOHN LEG\n",
      "  107: JOHN *IX *ARRIVE *MARY *JOHN                                  JOHN POSS FRIEND HAVE CANDY\n",
      "  108: *JOHN *BOOK                                                   WOMAN ARRIVE\n",
      "  113: IX CAR *IX *JOHN *BUY1                                        IX CAR BLUE SUE BUY\n",
      "  119: *JOHN *BUY1 IX CAR *IX                                        SUE BUY IX CAR BLUE\n",
      "  122: JOHN *GIVE1 BOOK                                              JOHN READ BOOK\n",
      "  139: JOHN *BUY1 WHAT *JOHN BOOK                                    JOHN BUY WHAT YESTERDAY BOOK\n",
      "  142: JOHN BUY YESTERDAY WHAT BOOK                                  JOHN BUY YESTERDAY WHAT BOOK\n",
      "  158: LOVE JOHN WHO                                                 LOVE JOHN WHO\n",
      "  167: JOHN IX *IX LOVE MARY                                         JOHN IX SAY LOVE MARY\n",
      "  171: JOHN *JOHN BLAME                                              JOHN MARY BLAME\n",
      "  174: *JOHN *GIVE1 GIVE1 *JOHN *BLAME                               PEOPLE GROUP GIVE1 JANA TOY\n",
      "  181: JOHN *JOHN                                                    JOHN ARRIVE\n",
      "  184: *IX BOY *GIVE1 TEACHER *JOHN                                  ALL BOY GIVE TEACHER APPLE\n",
      "  189: JOHN *IX *JOHN *WHAT                                          JOHN GIVE GIRL BOX\n",
      "  193: JOHN *IX *IX BOX                                              JOHN GIVE GIRL BOX\n",
      "  199: *JOHN CHOCOLATE WHO                                           LIKE CHOCOLATE WHO\n",
      "  201: JOHN *IX *IX *JOHN BUY HOUSE                                  JOHN TELL MARY IX-1P BUY HOUSE\n"
     ]
    }
   ],
   "source": [
    "model_selector = SelectorBIC\n",
    "\n",
    "models = train_all_words(features, model_selector)\n",
    "test_set = asl.build_test(features)\n",
    "probabilities, guesses = recognize(models, test_set)\n",
    "show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Testing the models selected using DIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** WER = 0.47752808988764045\n",
      "Total correct: 93 out of 178\n",
      "Video  Recognized                                                    Correct\n",
      "=====================================================================================================\n",
      "    2: JOHN *BUY *ARRIVE                                             JOHN WRITE HOMEWORK\n",
      "    7: JOHN *CAR GO *JOHN                                            JOHN CAN GO CAN\n",
      "   12: JOHN CAN *GIVE1 CAN                                           JOHN CAN GO CAN\n",
      "   21: JOHN *WHO *JOHN *JOHN *WHAT *GIVE1 *FUTURE *MARY              JOHN FISH WONT EAT BUT CAN EAT CHICKEN\n",
      "   25: JOHN *JOHN *LOVE IX IX                                        JOHN LIKE IX IX IX\n",
      "   28: JOHN LIKE IX *JOHN IX                                         JOHN LIKE IX IX IX\n",
      "   30: JOHN *MARY IX IX IX                                           JOHN LIKE IX IX IX\n",
      "   36: MARY *JOHN *IX IX *MARY *MARY                                 MARY VEGETABLE KNOW IX LIKE CORN1\n",
      "   40: JOHN IX *CORN MARY *MARY                                      JOHN IX THINK MARY LOVE\n",
      "   43: JOHN *JOHN BUY HOUSE                                          JOHN MUST BUY HOUSE\n",
      "   50: *JOHN JOHN BUY CAR *JOHN                                      FUTURE JOHN BUY CAR SHOULD\n",
      "   54: JOHN *FUTURE *FUTURE BUY HOUSE                                JOHN SHOULD NOT BUY HOUSE\n",
      "   57: JOHN *JOHN *IX MARY                                           JOHN DECIDE VISIT MARY\n",
      "   67: JOHN *YESTERDAY *WHO BUY HOUSE                                JOHN FUTURE NOT BUY HOUSE\n",
      "   71: JOHN *FUTURE VISIT MARY                                       JOHN WILL VISIT MARY\n",
      "   74: *IX *MARY *IX MARY                                            JOHN NOT VISIT MARY\n",
      "   77: *JOHN BLAME MARY                                              ANN BLAME MARY\n",
      "   84: *JOHN *JOHN *GIVE1 BOOK                                       IX-1P FIND SOMETHING-ONE BOOK\n",
      "   89: JOHN *JOHN *THROW *WOMAN IX NEW *BOOK                         JOHN IX GIVE MAN IX NEW COAT\n",
      "   90: JOHN *IX IX *IX WOMAN BOOK                                    JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   92: JOHN *IX IX *WOMAN *JOHN BOOK                                 JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "  100: POSS NEW CAR BREAK-DOWN                                       POSS NEW CAR BREAK-DOWN\n",
      "  105: JOHN *POSS                                                    JOHN LEG\n",
      "  107: JOHN *IX *ARRIVE *MARY *JOHN                                  JOHN POSS FRIEND HAVE CANDY\n",
      "  108: *JOHN *JOHN                                                   WOMAN ARRIVE\n",
      "  113: IX CAR *IX *JOHN *JOHN                                        IX CAR BLUE SUE BUY\n",
      "  119: *JOHN *BUY1 IX CAR *IX                                        SUE BUY IX CAR BLUE\n",
      "  122: JOHN *GIVE1 BOOK                                              JOHN READ BOOK\n",
      "  139: JOHN *ARRIVE WHAT *JOHN BOOK                                  JOHN BUY WHAT YESTERDAY BOOK\n",
      "  142: JOHN BUY YESTERDAY WHAT BOOK                                  JOHN BUY YESTERDAY WHAT BOOK\n",
      "  158: LOVE JOHN WHO                                                 LOVE JOHN WHO\n",
      "  167: JOHN IX *IX LOVE MARY                                         JOHN IX SAY LOVE MARY\n",
      "  171: JOHN *JOHN BLAME                                              JOHN MARY BLAME\n",
      "  174: *JOHN *GIVE1 GIVE1 *JOHN *CAN                                 PEOPLE GROUP GIVE1 JANA TOY\n",
      "  181: JOHN *JOHN                                                    JOHN ARRIVE\n",
      "  184: *IX *IX *GIVE1 TEACHER *JOHN                                  ALL BOY GIVE TEACHER APPLE\n",
      "  189: JOHN *IX *JOHN *JOHN                                          JOHN GIVE GIRL BOX\n",
      "  193: JOHN *IX *IX BOX                                              JOHN GIVE GIRL BOX\n",
      "  199: *JOHN *ARRIVE WHO                                             LIKE CHOCOLATE WHO\n",
      "  201: JOHN *IX *IX *JOHN BUY HOUSE                                  JOHN TELL MARY IX-1P BUY HOUSE\n"
     ]
    }
   ],
   "source": [
    "model_selector = SelectorDIC\n",
    "\n",
    "models = train_all_words(features, model_selector)\n",
    "test_set = asl.build_test(features)\n",
    "probabilities, guesses = recognize(models, test_set)\n",
    "show_errors(guesses, test_set)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nbpresent": {
   "slides": {
    "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a": {
     "id": "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a",
     "prev": null,
     "regions": {
      "3fb9ce83-fbb2-4995-832a-f8f400734ad3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1dbb9346-179b-4835-b430-6369d88f1a1b",
        "part": "whole"
       },
       "id": "3fb9ce83-fbb2-4995-832a-f8f400734ad3"
      }
     }
    },
    "1519a4fa-1588-4644-98de-9c43bf0aceb5": {
     "id": "1519a4fa-1588-4644-98de-9c43bf0aceb5",
     "prev": "8a712017-49b7-449f-8264-43a032ace902",
     "regions": {
      "29546121-ed11-44b7-8144-0c44e874098f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "365590a4-6963-4812-a1cf-688f7b6bb9ff",
        "part": "whole"
       },
       "id": "29546121-ed11-44b7-8144-0c44e874098f"
      }
     }
    },
    "176eaccb-15dd-455d-bf07-504213e7aa01": {
     "id": "176eaccb-15dd-455d-bf07-504213e7aa01",
     "prev": "de6b30f4-2463-4901-92ed-aabad78e5e0f",
     "regions": {
      "1542aa9e-dc55-4b90-adef-bf5181872b42": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c242050-c1f7-4b3b-8103-2ea9d71a40dc",
        "part": "whole"
       },
       "id": "1542aa9e-dc55-4b90-adef-bf5181872b42"
      }
     }
    },
    "19091b36-b0e7-49b1-b501-ec05937e0da9": {
     "id": "19091b36-b0e7-49b1-b501-ec05937e0da9",
     "prev": "1983c02e-fb99-4c05-a728-e0c0ad7c06d8",
     "regions": {
      "6529a31c-8d45-425c-b1d7-d0ac6fca6a32": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e766909d-9421-4aaf-9fb1-bc90d27e49e3",
        "part": "whole"
       },
       "id": "6529a31c-8d45-425c-b1d7-d0ac6fca6a32"
      }
     }
    },
    "1983c02e-fb99-4c05-a728-e0c0ad7c06d8": {
     "id": "1983c02e-fb99-4c05-a728-e0c0ad7c06d8",
     "prev": "176eaccb-15dd-455d-bf07-504213e7aa01",
     "regions": {
      "1c4e605d-7f22-4f30-b3fb-74b2937e7a4a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4d217204-e5c0-4568-bd30-12c2e41b681d",
        "part": "whole"
       },
       "id": "1c4e605d-7f22-4f30-b3fb-74b2937e7a4a"
      }
     }
    },
    "212b111f-4527-459c-8297-1db5580ee5c9": {
     "id": "212b111f-4527-459c-8297-1db5580ee5c9",
     "prev": "76898529-e49e-4663-8d02-8261dfe1d94b",
     "regions": {
      "2e4bd280-3cd6-47d0-9c81-17737b24053b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0c316996-9933-4b3d-82ec-259518dc8bc9",
        "part": "whole"
       },
       "id": "2e4bd280-3cd6-47d0-9c81-17737b24053b"
      }
     }
    },
    "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579": {
     "id": "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579",
     "prev": "e76e9a02-54c1-4ec9-80fb-c611ed398122",
     "regions": {
      "b5721d20-d6f8-4ddb-a5aa-eb16f0cc8893": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "313015a2-b5a9-4136-a8ea-5d011e47d840",
        "part": "whole"
       },
       "id": "b5721d20-d6f8-4ddb-a5aa-eb16f0cc8893"
      }
     }
    },
    "732f1952-ee54-46fb-8067-099512824296": {
     "id": "732f1952-ee54-46fb-8067-099512824296",
     "prev": "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a",
     "regions": {
      "f31d4597-08ad-4c46-ad52-4bd2d775c624": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "aadfec52-27ca-4541-8920-fa9253d51827",
        "part": "whole"
       },
       "id": "f31d4597-08ad-4c46-ad52-4bd2d775c624"
      }
     }
    },
    "76898529-e49e-4663-8d02-8261dfe1d94b": {
     "id": "76898529-e49e-4663-8d02-8261dfe1d94b",
     "prev": "19091b36-b0e7-49b1-b501-ec05937e0da9",
     "regions": {
      "ec1746fc-aec9-4a7c-8225-9e9ac8d45889": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b3e539be-84e2-49ce-a183-31cfc5c7ce7c",
        "part": "whole"
       },
       "id": "ec1746fc-aec9-4a7c-8225-9e9ac8d45889"
      }
     }
    },
    "8a712017-49b7-449f-8264-43a032ace902": {
     "id": "8a712017-49b7-449f-8264-43a032ace902",
     "prev": "bed9e696-630e-4747-be1c-bc3737ba992f",
     "regions": {
      "1faab517-cd16-4c63-bb01-a67246749d7a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3f14ddf0-4145-4687-9c33-712c3c32520f",
        "part": "whole"
       },
       "id": "1faab517-cd16-4c63-bb01-a67246749d7a"
      }
     }
    },
    "90af992d-eb6d-4496-b2d2-6aa9a95b6a61": {
     "id": "90af992d-eb6d-4496-b2d2-6aa9a95b6a61",
     "prev": "732f1952-ee54-46fb-8067-099512824296",
     "regions": {
      "4f448bec-5be9-4553-88ae-e35ed7612f25": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c445fbfb-b8ab-4e9a-8d13-12231a1c588f",
        "part": "whole"
       },
       "id": "4f448bec-5be9-4553-88ae-e35ed7612f25"
      }
     }
    },
    "bed9e696-630e-4747-be1c-bc3737ba992f": {
     "id": "bed9e696-630e-4747-be1c-bc3737ba992f",
     "prev": "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579",
     "regions": {
      "ac1513f0-404f-492b-8b42-0313e9a753b0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "18dd2eee-8b6c-4a5e-9539-132d00a7c7e1",
        "part": "whole"
       },
       "id": "ac1513f0-404f-492b-8b42-0313e9a753b0"
      }
     }
    },
    "de6b30f4-2463-4901-92ed-aabad78e5e0f": {
     "id": "de6b30f4-2463-4901-92ed-aabad78e5e0f",
     "prev": "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0",
     "regions": {
      "55ec36e0-362f-4fd3-8060-7cee056039aa": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c3cf461e-4c9e-4dec-99d2-07bfa79cbe23",
        "part": "whole"
       },
       "id": "55ec36e0-362f-4fd3-8060-7cee056039aa"
      }
     }
    },
    "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0": {
     "id": "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0",
     "prev": "1519a4fa-1588-4644-98de-9c43bf0aceb5",
     "regions": {
      "4c1e9714-9ba0-45fd-8a2f-ef80a5c85c2e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6534d4dc-125f-47e6-a022-cf1e0d277174",
        "part": "whole"
       },
       "id": "4c1e9714-9ba0-45fd-8a2f-ef80a5c85c2e"
      }
     }
    },
    "e76e9a02-54c1-4ec9-80fb-c611ed398122": {
     "id": "e76e9a02-54c1-4ec9-80fb-c611ed398122",
     "prev": "90af992d-eb6d-4496-b2d2-6aa9a95b6a61",
     "regions": {
      "9491b84d-193b-40ff-9321-d21eb1ba88d4": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b64ec10e-fa9d-4f3f-907f-6799611ed6b1",
        "part": "whole"
       },
       "id": "9491b84d-193b-40ff-9321-d21eb1ba88d4"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
